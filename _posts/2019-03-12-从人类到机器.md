---
layout:     post
title:      "深度学习与计算机视觉-从人类到机器"
subtitle:   "1.从人类到机器"
date:       2019-03-12 12:40:00
author:     "魔峥"
header-img: "img/post-bg-2015.jpg"
catalog: true
mathjax: true

tags:
    - 深度学习与计算机视觉
    - 体系整理
---


# 从人类到机器
## 1 第一步：记忆与计算的模拟
我们在江苏卫视《最强大脑》中可以看到很多神人。中国雨人周玮，世界记忆冠军王峰，水哥王昱珩等等。他们分别展示了计算能力，短时记忆能力，微小观察力与想象力。
其实除了想象力，普通人通过计算机都能远超越他们的能力。金庸先生的《鹿鼎记》全文约三四百万字，用计算机存储不过3M-4M。压缩存储不到2M，相当于一张相片的存储空间大。即使是20年前的计算机配置也能轻易存储下来，而我们人类却很难背下来。在电子计算机创造之初，计算机的计算能力就已经超过普通人了。计算机还可以连接摄像头，只要摄像头分辨率足够高，计算机的观察能力也会超过普通人类。



| 智能种类     | 人类 vs 人工智能                                             |
| ------------ | ------------------------------------------------------------ |
| 记忆能力     | 人类记忆能力不如存储设备精确高，量级大                       |
| 观察能力     | 只要摄像头分辨率高，超过人类视觉不成问题                     |
| 计算逻辑能力 | 计算机的计算速度远远超过人类；计算机的逻辑计算不会错         |
| 想像学习能力 | 人类可以举一反三具有广泛想像学习能力，计算机需要对单一服务进行编程后才能执行 |

记忆，计算，观察这三种人类的能力可以很好地被计算机超越后，计算机好像应该获得更高的智商，但是事实绝不是这样。计算机没有自我产生认知的能力，因此机器也就不会产生想象力。没有想象力，也就没有对**新问题**的解决方法。
因此，在初期计算机编程时，我们必须制定很复杂的规则来使计算机尽可能地适应各种问题，让计算机看起来很智能。（如同下面使用switch case语句穷举条件，适用不同的计算）这就是人工智能的开始。



```c
switch(c)
	{
     	case '+': d=a+b;
           printf("计算值为 %d",d); break;
     	case '-': d=a-b;
           printf("计算值为 %d",d); break;
     	case '*': d=a*b;
           printf("计算值为 %d",d); break;
     	case '/': d=a/b;
           printf("计算值为 %d",d); break;
     	case '%': d=a%b;
           printf("计算值为 %d",d); break;
    }
```
这种穷举的方法直到今年（2019年）仍然是最常用，最简单那的方法。我们经常看见一段代码几十个，几百个if else语句。但这种做法很难令人满意。代码很可能经常重改，一旦重改就会重新编译；最重要的是：穷举常常是举不到终点的。

如何让计算机产生对新事物的认知（即认识不断出现的新生事物）是一件极其困难的事。**我们将“获得知识、技能或认知的过程”叫学习，“计算机得知识、技能或认知的过程”的过程称为机器学习。**

## 2 第二步：对“学习”的数据抽象
在百度百科词条上，[“学习”](https://baike.baidu.com/item/%E5%AD%A6%E4%B9%A0/222729?fr=aladdin)一词的解释如下：**学习是通过阅读、听讲、观察、研究、实践等途径而获得知识、技能或认知的过程。**
仔细研究这个解释，我们可以得到两个点：
  1. 学习是个“过程”。
  2. 在学习这个“过程”产生之前，我们还需要一些前置的“途径”（方法）。

我们每个人每天都在阅读、听讲、观察、研究、实践。每个人都会从这些过程中产生一些知识、技能或认知。但每个人产生的东西是有区别。
大家都知道美国总统特朗普，每个人对他都有一番评价。但我们不是天生就认识特朗普。每个人的大脑结构是不变的，却可以对新生事物可以产生**有效的认知**。这种事情给予人极大的启发。我们是否可以编写类似不变的程序，该程序也可以**有效地处理新出现的问题**。
答案是肯定的：我们可以将事先不可知的新输入抽象整理为可计算数据，然后通过数据计算转化为输出。
我们可以用这样的程序，将特朗普总统数据化：
```
初始化特朗普得分（这里输入是特朗普）：
score = 0
特朗普反建制，反精英：
score = score + 20*参数1
特朗普修墙：
score = score - 10*参数2
特朗普歧视女性：
score = score - 10*参数3
特朗普要打贸易战：
score = score - 10*参数4
if（score < 0）特朗普是个带恶人
	else 特朗普是个好总统
```
上面伪代码中，我们将某人的行为与立场转化为各种数据，这种转化不怕新人物的输入，都可以给出问题答案。。但是问题同时出现了。上面的参数怎么确定？不同人会给出不同答案。不同的答案会使得结果完全相反。
有些人自己提出参数设置，计算出“特朗普是个带恶人”；而有些人给出的参数却计算出“特朗普是美国历史上最好总统”。不同人在不同的生活环境会产生不同的参数。这里的参数就是人们的认知。我们可以想象。天天听“This is extremely dangerous to our democracy.”的青年人对特朗普总统反感是很正常的；而不相信精英的反建制派们，支持特朗普也是很正常的。
那么我可以得到以下结论：如果假设人类认知形成的原理相同，那么即使面对同一种人（或事物），在不同的环境下形成认知的人们立场与态度很可能完全不同。我们还可以抽象成计算机语言：当计算机代码执行方式是固定的，在面对相同的新输入时，不同人设置参数的程序输出很可能不同。

那么作为一个外国人，我到底听谁说的话呢？（我到底信谁的参数呢？）砖家啊！！！对，这种“**内部含有大量的某个领域专家水平的知识与经验，能够利用人类专家的知识和解决问题的方法来处理该领域问题**”就叫“**专家系统**”。

## 3 第三步：仿生学的人工智能

个人认为，仿生其实是一种思考方式。当我们讨论飞上天的办法时，首先是考虑“鸟是怎么飞上天的”这一问题。这种思路可以带到人工智能里，通过研究人脑的原理，设计出具有人工智能的机器。

在人工智能里面，我们有一个最终目的：设计出一个机器（或数学模型）仿照大脑，使机器具有类似人脑的“**学习能力**”。1943年，心理学家Warren McCulloch和数理逻辑学家Walter Pitts在合作的《A logical calculus of the ideas immanent in nervous activity》论文中提出并给出了人工神经网络的概念及人工神经元的数学模型，从而开创了人工神经网络研究的时代。 1957年，就职于Cornell航空实验室(Cornell Aeronautical Laboratory)的Frank Rosenblatt基于前人的理论，发明了一种最简单形式的前馈式人工神经网络，即“Rosenblatt感知机”。它可以看成是我们神经网络开端。

###  3.1 从神经元到感知机
学过高中生物课程的人一定都记得这张图。
![](\img\mozheng\大脑中神经元.png)
这就是神经元细胞的结构。一端接收数据，一端传输数据。多个神经元组合在一起，下一个神经元接收到信息，如果接收到的递质信号量足够大，可以引起刺激，信号才会向下传递。神经信号，传输过程如下面的图
![](\img\mozheng\传递递质.png)

我们仿照神经元机制，制造一个这样的神经元：人工神经元一端接收数据一端传输数据，多个人工神经元可以组合在一起，当下一个神经元接收到的信号大于某一个阙值θ时，正向的y才能输出。下图就是根据这样的思想发明出来的M-P模型。
![](\img\mozheng\仿生学模拟神经元.png)

$$
y_i = f(\sum\limits_{i=1}^n w_{ij}x_i-\theta)
$$

这里f(.)是激活函数。使用这个人工神经元的方法很简单，我们将大量的例子“喂”给它。让它自我消化，更新出来一系列神经元内部的参数。这个过程就是“**机器学习**”
此时，单独的一个M-P模型似乎没什么用。但是伴随着**感知机的收敛性（Novikoff定理）**这个**机器学习算法**。感知机立刻变得受人追捧起来。感知机收敛算法概述如下：

#### 感知机收敛算法概述

设有一线性二元可分数据集，数据集的输出结果非黑即白（用-1与+1表示），且存在一个超平面将这两种完美分开。输入向量x(n)为二元可分子集，对应的y(n)便签{-1,+1}中+1的一方。
参数变量为：

$$
\begin{equation}\begin{split}
&x(n)=[+1,x_1(n),x_2(n)\cdots,x_m(n)]^T:{m+1维输入向量,n为第n次迭代} \\
&w(n)=[b,w_1(n),w_2(n)\cdots,w_m(n)]^T:{m+1维权值向量,n为第n次迭代} \\
&y(n):{实际响应,结果只有-1与+1二值}\\
&d(n):{期望相应,结果只有-1与+1二值}\\
&\eta:{学习率，比1小的正常数}
\end{split}\end{equation}
$$

1. 开始计算第n次迭代权重值w(n)。如果n=0，初始化第n=0次迭代的权重值，w(0)=0.
2. 计算实际响应y(n)，其中sgn(.)是符号函数：
$$
y(n)=sgn[w^T(n)x(n)]
$$

3. 如果分类出错则更新向量值w(n+1)，否则不更新：
$$
w(n+1)=w(n)+\eta[d(n)-y(n)]x(n)
$$

4. 如果w(n)与w(n+1)的误差在容许范围内，停止计算；否则回到步骤1

这里我们还要证明迭代收敛：在有限步骤内可以使w(n)与w(n+1)相等（这里的数据集是线性可分的）。
#### 感知机收敛证明

首先假设前提，输入向量x(n)为二元可分子集O，对应的y(n)便签{-1,+1}中+1的一方。

$$
\eta[d(n)-y(n)]=1, w(0)=0 \\
\therefore w(n+1)=x(1)+x(2)+\cdots+x(n)
$$

当第n次为感知机不能正确分类时，修改此时变量权重，有
$$
w^T(n)x(n)<0
$$

前提说明结束证明开始，该证明分两段：

1. 第一段

$$
\begin{equation}\begin{split}
& 设 \ \exists w_0,\alpha,使得 w_0x(n)>0,且a=\min_{x(n) \in O }w^Tx(n) \\
&\therefore w_0^Tw(n+1) \geqslant n\alpha \\
&\because Cauchy-Schwarz不等式，\|w_0\|^2 \|w(n+1)\|^2 \geqslant \|w_0^Tw(n+1)\|^2\\
&\therefore \|w_0\|^2 \|w(n+1)\|^2 \geqslant n^2\alpha^2 \\
&\therefore \|w(n+1)\|^2 \geqslant \frac{n^2\alpha^2}{\|w_0\|^2}
\end{split}\end{equation}
$$

2. 第二段

$$
\begin{equation}\begin{split}
&\|w(n+1)\|^2 = \|w(n)+x(n)\|^2 = \|w(n)\|^2 + \|x(n)\|^2 + 2\|w(n)\|\|x(n)\| \\
&\because w^T(n)x(n)<0 \\
&\therefore \|w(n+1)\|^2 \leqslant \|w(n)\|^2 + \|x(n)\|^2 \\
&\therefore \|w(n+1)\|^2 - \|w(n)\|^2 \leqslant  \|x(n)\|^2 \\
\end{split}\end{equation}
$$

看见没？高中数学推到这里就该累加了。

$$
\begin{equation}\begin{split}
&\therefore \|w(n+1)\|^2  \leqslant \sum\limits_{k=1}^n \|x(k)\|^2 \\
& set: \beta=\max_{x(k) \in O}\|x(k)\|^2 \\
&\therefore \|w(n+1)\|^2  \leqslant n\beta \\
&\therefore \frac{n^2\alpha^2}{\|w_0\|^2} \leqslant \|w(n+1)\|^2 \leqslant n\beta \\
&\therefore n \leqslant \frac{\beta\|w_0\|^2}{\alpha^2} \\
\end{split}\end{equation}
$$

所以，我们发现当n的更新次数会停止，迭代次数n是有上限的，该算法会收敛。

由上面的推论可以得到，如果是一个线性可分的数据集，我是可以在有限次更新后将数据集完美分割。从某种意义上讲，这样的结果就是上面说的“超平面”。这条经典的定理解释了“感知机为什么会有效”。“机器学习”发展到今天，一次完整训练可能需要几十台机器几个月的时间，如果没有这条收敛定理，给了我们继续训练下去的理由。

感知机看起来是个简单的模型，不用通篇数学公式与证明，但它的发展并不那么顺利。而人工神经元的发展更是一波三折。在相当长的时间里，我们可能只知道支持向量机（SVM）而忘了人工神经元。直到经历过三次危机之后。

### 3.2 从单层神经网络到深度学习

#### 第一次危机：单层感知机不能解决“异或”逻辑问题

之前我一直强调“**线性可分数据集**”。其实我特意忽视了一点：数据集不可分怎么办？如果两类有交杂部分，且越往外面两类区分越明显。我们可以使用“**损失函数**”这一概念。定义一个误分类损失函数，然后我们最小化这个损失函数。这就是抗战剧中常说的“尽可能将伤害损失控制到最小”。
![](\img\mozheng\SVM软间隔.gif)
但是有一种很简单的小东西比较恶心，那就是简单的异或问题。。如图
![](\img\mozheng\异或问题.jpg)
我们真的不能画一条线看起来比较好的线来分割。而且这不仅仅是感知机一家的问题， 所有的线性分类器都有这样的问题，包括LDA(Linear discriminant analysis), linear-SVM， Logistic regression都不能做XOR。数学理论是严谨的，但使用数学工具的人是灵活的。我们又发现，具有一个隐藏层组织的神经网络可以解决异或这个问题。我们计算一下:
![](\img\mozheng\两个神经元.png)
这里的sgn后来演化成激活函数。多层的神经元将感知机的研究直接拉入**多层人工神经网络时代**。

#### 第二次危机：多层人工神经网络问题

“异或”问题似乎解决了。但却暴露了很大的问题？
1. 人工神经网络是否可以适用所有的场景？
2. 多层神经网络怎么进行参数训练？

第二个问题很快就解决了。1974年Paul Werbos于博士论文中提出BP算法初步解决了多层神经网络的训练问题，1985年由Rumelhart等人发展了该理论。BP算法比较复杂，总结起来就是前向计算，反向求导。
而第一个问题却迟迟不能解决，直到1989年**通用近似定理（universal approximation theorrm）(Hornik et al., 1989;Cybenko, 1989)** 的发表。在这个定理表明：“一个包含足够隐含层神经元的多层前馈网络，能以任意精度逼近任意预定的连续函数。”
这个定理的原文太复杂，这里可以谈下里面的关键点：

1. 我们可以设计一个网络，“以任意精度逼近”而不是准确计算。增加隐含层可以提升近似精度。
2. 被近似的函数必须是连续的。如果是非连续的就爱莫能助了。例如这个函数
$$
\begin{equation}
f(x)=\left\{
\begin{aligned}
-1 \ &(x是无理数) \\
1 \ &(x是有理数)
\end{aligned}
\right.
\end{equation}
$$

神经网络往深度层次发展的方向已经有了理论基础。同时该理论又被发展为：三层神经网络（带有隐层）可以以任意精度逼近任意预定的连续函数。这条定理也给出了神经网络可以往广度发展的答案。但不管怎么讲，当时进入“**多层人工神经网络时代**”的前置数学理论已经建成。但实际情况却不这样，从1989年到2014年，一直拖了25年。主要原因是遭遇了"人工神经网络"的第三次危机：多层人工神经网络计算量爆炸问题。

#### 第三次危机：多层人工神经网络计算量爆炸问题
神经网络的第三次危机源于计算机算力不足。“通用近似原理”需要叠加神经元层数，BP算法里面有大量求导操作，使得二十世纪九十年代的计算机不堪承受如此的重载。很快人工神经网络的发展再次陷入低潮，一直没被重视。直到2014年Hinton和他的学生Alex Krizhevsky发表AlexNet。不管从是算法的简洁度，结果的准确度都完胜第二名。也是在那年之后，更多的更深的神经网路被提出，比如优秀的VGG，GoogLeNet。AlexNet的最大贡献便是**卷积神经网络(Convolutional Neural Networks, CNN)**的提出。
1. CNN改进了以往神经网络连接的方法，将全连接方法为主改为更适合图片的卷积连接方法为主。这种改动大大地减少了运算量。
2. NVIDIA之前就有图像模板的优化计算，AlexNet利用多块NVIDIA显卡对大量的模板计算进行加速。

当然了，全球游戏高端玩家，比特币矿机，AMD的追赶，间接推进NVIDIA进行高并行GPU的快速升级。事实上，“计算爆炸问题”依然没有被消除，而是被“计算机运算能力加强”与“全连接模型简化”两个因素减缓了。但是，就是这一点点减缓，却促使机器学习的精度上了一个台阶。在1943年沃伦·麦克洛克（Warren McCulloch）开创M-P神经元模型的71年后，Hinton和他的学生Alex Krizhevsky带着AlexNet重回人们的视线，带领人类进入“**深度学习时代**“。


# 雄关漫道真如铁，而今迈步从头越